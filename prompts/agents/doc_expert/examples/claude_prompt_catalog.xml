<example_prompt_engineering_hub>
    <root_readme>
        ![Prompt Engineering Hub](data/prompt-engineering-hub-logo.png)

        [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
        [![GitHub Release](https://img.shields.io/github/release/ConsciousML/prompt-engineering-hub.svg?style=flat)]()
        [![XML Validation](https://github.com/ConsciousML/prompt-engineering-hub/actions/workflows/ci.yaml/badge.svg)](https://github.com/ConsciousML/prompt-engineering-hub/actions/workflows/ci.yaml)
        [![PR's Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat)](http://makeapullrequest.com) 
        # Prompt Engineering Hub

        A prompt hub to help you get the most out of your favorite LLM by generating (or using ready-made) optimized prompts!

        ## What's Inside

        ### The Prompt Generator
        An assistant that helps you create optimized prompts for any task, following the prompt engineering best practices of [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview), [OpenAI](https://platform.openai.com/docs/guides/) and [Google's Gemini](https://ai.google.dev/gemini-api/docs/prompting-strategies).

        ### Ready-Made Assistants
        A [collection of prompts](#prompt-catalog) for a variety of tasks. All created using our Prompt Generator

        ## When to Use
        Use this repository whether you want to:
        - Create your own custom AI assistants (or user prompts)
        - Use our pre-built assistants
        - Learn from prompt engineering examples
        - Contribute your own prompts to help the community!

        ## Requirements
        Any LLM chat interface (Claude, ChatGPT, Gemini, etc.) or APIs

        **Note**: While these prompts work across different LLMs, they were optimized using Claude and may need minor adjustments for other platforms.

        ## Quick Start
        See our [setup guide](docs/setup-guide.md) for detailed instructions on using these prompts.

        ## Advanced Prompt Generation Guide
        **Create your own assistants** using our [advanced prompt generation documentation](prompt_generator/README.md#advanced-prompt-generation-guide).

        This guide walks you through an iterative workflow to create optimized prompts for your specific tasks.

        ## Prompt Catalog
        Here's a table with available prompts:
        | Assistant | Usage | Description | Status |
        |--------|-------|-------------|--------|
        | [**Prompt Generator**](prompt_generator/README.md) | Prompt Engineering | Generates optimized prompts using advanced techniques like chain-of-thought, prompt chaining, and XML formatting. Follows Anthropic's best practices. | ![Stable](https://img.shields.io/badge/status-stable-green) |
        | [**Example Generator**](prompts/example_generator/README.md) | Prompt Engineering | Creates XML examples that demonstrate assistant behavior to improve performance. | ![Stable](https://img.shields.io/badge/status-stable-green) |
        | [**Code Documentation Expert**](prompts/doc_expert/README.md) | Documentation | Creates README files and code docstrings through an iterative, collaborative process. Guides you through information gathering, outline creation, and section-by-section writing for READMEs, or generates appropriate docstrings for your code. | ![Beta](https://img.shields.io/badge/status-beta-yellow) |
        | [**Mermaid Diagram Designer**](prompts/diagram_designer/README.md) | Diagrams | Builds clear, well-structured diagrams using Mermaid syntax. It automatically selects the most appropriate diagram type for your needs and follows best practices for visual clarity. | ![Beta](https://img.shields.io/badge/status-beta-yellow) |
        | **Insight Extractor** | Research | Extracts key findings from articles, research papers, forums, and other content sources. Includes source referencing with text fragment linking for verification. | ![Experimental](https://img.shields.io/badge/status-experimental-red) |
        | **Insight Consolidator** | Research | Takes the out put of the Insight Extractor. Curates every insight to answer a user query. Preserves the text fragment urls. | ![Experimental](https://img.shields.io/badge/status-experimental-red) |
        | **Community Insight Analyst** | Research | Extracts insights from community feedback reports, organizing findings into wants, frustrations, objections, and misunderstandings. Every insight is backed by direct quotes. | ![Experimental](https://img.shields.io/badge/status-experimental-red) |

        For guidance on how to use an assistant, click on the respective link under the `Assistant` tab.

        ## Repository Structure
        ```bash
        prompt-generator-hub/
        ├── prompt_generator/                     # The main prompt generation tool
        │   ├── system.xml
        │   ├── examples/
        │   │   └── example_1.xml
        │   │   └── ...
        │   └── user_facing_prompts/
        │       └── evaluate_insights.xml
        ├── prompts/                              # Ready-made assistants created with our generator
        │   ├── example_generator/
        │   │   ├── system.xml
        │   │   ├── examples/
        │   │   └── user_facing_prompts/
        │   ├── doc_expert/
        │   │   └── ...
        │   ├── diagram_designer/
        │   │   └── ...
        │   └── insight_extractor/
        │       └── ...
        └── docs/
            ├── setup-guide.md
            └── contribution.md
        ```

        Here's a brief description of each file type:
        - `system.xml`: system prompts for Custom Projects or API. Copy these into Project Instructions or use with your LLM's system prompt feature.
        - `user_facing_prompts/`: ready-to-use prompts for direct conversation. Copy and paste into any LLM chat.
        - `example_*.xml`: example files demonstrating the assistant's behavior.

        Should I continue with the Contributing section?

        ## Contributing
        We welcome contributions from the community! Help us grow this collection by sharing your prompts.

        See the [contributing documentation](docs/contribution.md) for detailed guidelines.

        Thank you for helping make AI assistants more useful for everyone!

        ## License
        This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

        Feel free to use, modify, and distribute these prompts in your own projects!
    </root_readme>
    <contrib_sub_readme>
        # Contribution Guidelines
        Please familiarize yourself with [Anthropic's Prompt Engineering Documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview) to ensure your prompts follow best practices.

        ## Submission Guidelines

        1. Create a new folder under `prompts/` with a descriptive name (e.g., `data_analyzer`, `meeting_summarizer`)
        2. Include a `system.xml` file with your system prompt
        3. Add `user_facing_prompts/` folder if your prompt includes prompt templates.
        4. Follow XML formatting as shown in existing prompts
        5. Test your prompt thoroughly before submitting

        ## File Structure Requirements
        ```
        prompts/your_prompt_name/
        ├── system.xml
        └── user_facing_prompts/          # Optional: Conversational examples
        ├── examples/
        |   └── example_1.xml
        |   └── ...
        ```

        ## How to Submit
        1. Fork this repository
        2. Create a new branch for your prompt
        3. Add your prompt following the guidelines above
        4. Update this README's Prompt Catalog table
        5. Submit a pull request with a clear description

        Thank you for helping make AI assistants more useful for everyone!
    </contrib_sub_readme>
    <setup_guide_sub_readme>
        # Prompt Setup Guide
        This documentation shows you how-to use the custom prompts from this catalog.
        
        ## Option 1: Create a Custom Project (Recommended)
        Use the Projects feature to create assistants that you can re-use and have all the context readily available.
        
        Here's how to proceed depending on which LLM chat interface you use.
        
        ### Claude Projects
        1. Go to the [Projects page](https://claude.ai/projects)
        2. Click on the `New project` button
        3. Write a name for the project (e.g., "Prompt Generator")
        4. On the right, below `Project Knowledge` click `Edit`
        5. Copy the content of `prompt_generator/system.xml`
        6. Paste into project instructions
        7. Drag and drop all the files in the `prompt_generator/examples/` folder
        8. See this [example](../prompt_generator/examples/readme_writer.xml) for guidance on how to use the Prompt Generator.
        9. Start a conversation by writing your first prompt in the text box below `Prompt Generator`.
        
        ### ChatGPT Projects
        1. Click on `New Project` in the left tab.
        2. Write a name for the project (e.g., "Prompt Generator").
        3. Click the `Create project` button.
        4. Click the `Add files` button.
        5. Drag and drop the following files in the Web UI:
        - `prompt_generator/system.xml`.
        - All the files in the `prompt_generator/examples/` folder.
        6. See this [example](../prompt_generator/examples/readme_writer.xml) for guidance on how to use the Prompt Generator.
        7. Start a conversation by writing your first prompt in the text box below `Prompt Generator`.
        
        ### Gemini Gem
        1. Go to the [New Gem page](https://gemini.google.com/gems/create).
        2. Write a name for the project (e.g., "Prompt Generator").
        3. Copy the content of `prompt_generator/system.xml` under `Instructions`.
        4. Drag and drop all the files in the `prompt_generator/examples/` folder under `Knowledge`.
        5. Click `Save`.
        6. See this [example](../prompt_generator/examples/readme_writer.xml) for guidance on how to use the Prompt Generator.
        7. Start a conversation.
        
        ## Option 2: Use in Regular Conversations
        Use this option for quick one-off tasks or if you don't have a subscription.
        
        For system prompts (without subscription):
        1. Copy the contents of any the `prompt_generator/system.xml` file
        2. Paste directly at the start of a conversation
        3. See this [example](../prompt_generator/examples/readme_writer.xml) for guidance on how to use the Prompt Generator.
        
        For user-facing prompts:
        1. Copy the contents of any file in `user_facing_prompts/` folders
        2. Paste directly into a conversation
        3. Replace placeholders like `[SPECIFIC_TASK]` with your details
    </setup_guide_sub_readme>
    <prompt_gen_sub_readme>
        # Prompt Generator
        The Prompt Generator is the core assistant of this hub. It was used to creates all other assistants.

        It was designed to guide you through the process of creating optimized prompts for your workflow.

        When to start right away? Follow the [quickstart](#quick-start) or [advanced guide](#advanced-prompt-generation-guide).

        ## Why Use the Prompt Generator?

        The Prompt Generator knows the best practices from the prompt engineering documentation of [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview), [OpenAI](https://platform.openai.com/docs/guides/) and [Google's Gemini](https://ai.google.dev/gemini-api/docs/prompting-strategies). It creates prompts like an expert prompt engineer.

        Key benefits:
        - Get professional quality prompts without being an expert
        - Save time, no need to learn complex prompting techniques
        - Create prompts that get better results from LLMs

        ## When to Use the Prompt Generator?

        The Prompt Generator is most valuable in these situations:

        - **For tasks you do regularly with LLMs**: Create a specialized assistant once, then reuse it instead of explaining the task every time
        - **When your LLM default responses aren't good enough**: The Prompt Generator adds techniques that improve LLM performance on difficult tasks
        - **When you need specific behavior from your LLM**: Control the reasoning process, output format, or step-by-step approach the LLM should follow
        - **When your task requires lots of context or specifications**: Instead of providing lengthy instructions each time, create an assistant that remembers all requirements

        ## Prerequisites

        To get the most out of the Prompt Generator, we recommend reading [Anthropic's prompt engineering documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview).

        This will help you understand the techniques the Prompt Generator uses and have better conversations with it.

        However, if you're eager to start, you can use the Prompt Generator right away. It will guide you through the process and explain concepts as needed.

        ## How It Works 
        The Prompt Generator supports three modes:
        1. **Create from scratch**: Guides you through creating a new prompt with targeted questions about your needs
        2. **Review existing prompts**: Analyzes your prompts to:
        - Follow best practices
        - Avoid redundancies
        - Ensure every instruction is useful
        3. **Improve based on results**: Refines prompts based on real-world outputs and/or user feedback

        ## Prompt Techniques
        The Prompt Generator uses proven techniques to create effective prompts:
        - **System Prompts**: Creates role-based prompts that define the LLM's expertise and context, greatly improving performance for specialized tasks
        - **Chain-of-Thought (CoT)**: Adds thinking space for the LLM to break down problems step-by-step, leading to more accurate outputs
        - **Prompt Chaining**: Breaks complex tasks into smaller, manageable subtasks that build on each other
        - **XML Formatting**: Structures prompts with clear sections and tags for better context organization
        - **Minimum Viable Prompt (MVP)**: Starts with essential elements only, avoiding unnecessary complexity

        ## Quick Start
        Follow our [setup guide](../../docs/setup-guide.md) to get started with the Prompt Generator.

        ## Advanced Prompt Generation Guide
        This section will explain how to get the most of the Prompt Generator.

        Prompt engineering is an iterative process. Here's an overview of the proposed workflow:
        1. [Create an MVP](#step-1-create-minimum-viable-prompt-mvp): Start with the essential elements.
        2. [Review for issues](#step-2-review-the-prompt): Check adherence to best practices.
        3. [Refine the prompt](#step-3-improve-the-prompt): Address identified issues.
        4. [Test in real scenarios](#step-4-test-the-prompt): Use your assistant on actual tasks.
        5. [Improve based on results](#step-5-improve-the-prompt-based-on-outputs): Fix any performance gaps.
        6. [Add examples](#step-6-generate-examples): Boost performance with few-shot learning.

        ### Configuration Recommendation
        Generating optimal prompts is a complex task.
        To get the most out of this assistant, we recommend using one of these models:
        - Claude Opus 4 with [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)
        - ChatGPT o3
        - Gemini 2.5 Pro

        Here's a screenshot illustrating how to enable extended thinking on Claude:
        ![](../../data/extended_thinking.png)

        ### Step 1: Create Minimum Viable Prompt (MVP)
        Start from scratch to create a new prompt:
        1. Write all the details about the prompt you want to create.
        2. The assistant will ask clarification questions.
        3. Answer the questions.
        4. The assistant will generate an MVP (a first version of your prompt).
        5. Review the MVP carefully. Remove any unnecessary instructions.

        By this stage, you should end up with a relatively simple prompt.

        Here's an [example conversation](examples/readme_writer.xml) illustrating how to generate an MVP.

        ### Step 2: Review the Prompt
        The MVP is rarely perfect out of the box.

        Luckily, this assistant comes with a functionality that allows you to spot the issues of the initial prompt:
        1. Ask the assistant to review the prompt. Use a simple phrase like: "Review the MVP."
        2. The assistant will check that:
        - It follows all the guidance and constraints of the Anthropic's documentation.
        - It is free of redundancy.
        - Every instruction is useful for the engoal.
        3. The assistant will expose the issues of the MVP.

        ### Step 3: Improve the Prompt
        1. Approve, decline or change suggestions from the previous step.
        2. Feed the revised suggestions to the assistant and ask to improve the prompt.
        3. Repeat [step 2](#step-2-review-the-prompt) and [step 3](#step-3-improve-the-prompt) until you are satisfied with your MVP.

        **Caution**: By the end of this step. You should end up with a relatively simple prompt. If the prompt is too verbose, there's a high change that it will perform poorly.

        ### Step 4: Test the Prompt
        1. Perform the [setup guide](../docs/setup-guide.md) again with your newly created prompt to add the generated assistant to your workspace.
        2. Start a conversation and perform a real-world task with the assistant.
        3. Note any lack of performance or undesired behavior.

        ### Step 5: Improve the Prompt Based on Outputs
        1. Start a conversation with the Prompt Generator again.
        2. Feed him:
        - The system prompt of your generated assistant.
        - The query (user prompt) that illustrates the issues noticed in [step 4](#step-4-test-the-prompt).
        - Explain the issue and ask the Prompt Generator to suggest improvements.
        4. The Prompt Generator will suggest improvements.
        5. Accept, decline or change any suggested improvements.
        6. Ask the Prompt Generator to improve the prompt based on the revised suggestions.

        If you want to maximize the performance of your assistant, repeat this step each time you notice a major issue.

        ### Step 6: Generate Examples
        LLMs perform significantly better with examples (few-shot).

        If you successfully performed a task (you achieved the desired outcome) with your generated assistant, you can use your queries (user prompts) and the assistant's responses to create an example.

        When creating/generating examples, make sure that:
        - The example illustrates how the assistant should function.
        - Use XML formatting with `<user>` and `<assistant>` tags. See this [example](examples/readme_writer.xml).
        - If your assistant uses Chain of Thought (CoT), make sure to incorporate `<thinking>` tags in your example.
        - The example does not contradict any instruction from its system prompt.

        **Note**: If the assistant functions step-by-step, do not feed the final desired outcome. For example, if you create an assistant to write READMEs section by section, emulate this in the example.

        To make this process easier, you can use the [Example Generator](../prompts/example_generator/README.md).
    </prompt_gen_sub_readme>
    <example_gen_sub_readme>
        # Example Generator
        The Example Generator creates XML examples that demonstrate how your assistants should behave.
        
        ## Purpose
        Examples (few-short) are one of the most powerful tools for improving the performance of LLMs.
        
        Unfortunately, it can be long and tedious to create them. The Example Generator helps you facilitate this process.
        
        Here's in which context the assistant can be used:
        1. Notice an undesired behavior of an assistant.
        2. Use the Example Generator to create an example that illustrates the desired behavior.
        3. Feed the generated example to the initial assistant and test it again.
        
        This iterative process is part of the broader prompt improvement workflow described in [Prompt Generator step 6](../../prompt_generator/README.md#step-6-generate-examples).
        
        **Pro tip**: The most effective examples come from reformatting real conversations where your assistant performed exactly as intended.
        
        ## Quick Start
        Follow our [setup guide](../../docs/setup-guide.md) to get started with the Example Generator.
        
        ## How To Use
        ### Mode 1: Create Examples from Scratch
        1. Start a conversation by:
        - Giving the system prompt of the assistant you want to create examples for
        - Rename the main `&lt;system&gt;` tag to `&lt;system_[NAME_OF_YOUR_ASSISTANT]&gt;` to avoid confusion
        - Ask: "Let's create an example for the [NAME_OF_YOUR_ASSISTANT]"
        2. The Examples Generator asks what aspect of the system prompt you want to demonstrate
        3. Specify the behavior or workflow you want to show
        4. The assistant generates one user/assistant interaction
        5. Review and validate the example
        6. Continue with until the complete conversation
        
        ### Mode 2: Improve Existing Examples
        1. Start a new conversation and provide:
        - The system prompt of the assistant you want to create examples for
        - Rename the main `&lt;system&gt;` tag to `&lt;system_[NAME_OF_YOUR_ASSISTANT]&gt; to avoid confusion
        - The existing example
        2. Explain what improvements you need (clarity, better demonstration of features, fixing inconsistencies, etc.)
        3. The assistant generates an improved version
        4. Review and validate the improvements
        
        ### Mode 3: Refactor Existing Conversation
        1. Start a conversation by providing:
           - The system prompt of the assistant
           - A real conversation where your assistant performed well
        2. Ask: "Refactor this conversation into an example"
        3. The Examples Generator will:
           - Clean up the conversation flow
           - Add proper XML formatting
           - Include `&lt;thinking&gt;` tags if applicable
           - Remove any unnecessary parts
        4. Review the reformatted example
        5. Request adjustments if needed
        
        ## Tips on Examples
        
        - **Demonstrate complete workflows**: Show the entire process from user request to final output
        - **Use realistic scenarios**: Base examples on actual use cases rather than generated from scratch
        - **Include thinking tags**: If your assistant uses Chain of Thought (CoT), always include `<thinking>` tags in examples
        - **Show edge cases**: Create examples for tricky situations or common user mistakes
        - **Be consistent**: Ensure examples don't contradict any instructions in the system prompt
        - **Keep it focused**: Each example should demonstrate one clear aspect of the assistant's behavior
    </example_gen_sub_readme>
</example_prompt_engineering_hub>